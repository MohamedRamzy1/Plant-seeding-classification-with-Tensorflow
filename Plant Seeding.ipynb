{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image , ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x[:-1] for x in open(\"Labels\" , 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_num = {}\n",
    "num_to_labels = {}\n",
    "for x , y in enumerate(labels):\n",
    "    num_to_labels[x] = y\n",
    "    labels_to_num[y] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = pd.read_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = Images[:3800]\n",
    "Validation = Images[3800:]\n",
    "Train_X = Train['Image']\n",
    "Train_Y = Train['Labels'].as_matrix()\n",
    "Valid_X = list(Validation['Image'])\n",
    "Valid_Y = Validation['Labels'].as_matrix()\n",
    "\n",
    "Train_Y = tf.one_hot(Train_Y , 12)\n",
    "Valid_Y = tf.one_hot(Valid_Y , 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_minibatch(idx , size , xdata , ydata):\n",
    "    start = idx * size\n",
    "    if(start + size >= len(xdata)):\n",
    "        end = len(xdata)\n",
    "    else:\n",
    "        end = start + size\n",
    "    directory = \"TrainR/\"\n",
    "    x_batch = np.array(ImageOps.fit(Image.open(directory + xdata[start]) , (100,100) , Image.ANTIALIAS).convert('RGB')).reshape(1,100,100,3)\n",
    "    for ImgName in xdata[start+1:end]:\n",
    "        img = np.array(ImageOps.fit(Image.open(directory + ImgName) , (100,100) , Image.ANTIALIAS).convert('RGB')).reshape(1,100,100,3)\n",
    "        x_batch = np.concatenate((x_batch , img))\n",
    "    y_batch = ydata[start : end]\n",
    "        \n",
    "    return x_batch / 255.0 , y_batch*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype = tf.float32 , shape = [None , 100 , 100 , 3])\n",
    "Y = tf.placeholder(dtype = tf.float32 , shape = [None , 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(\"W1\" , shape = [3,3,3,8] , initializer =  tf.contrib.layers.xavier_initializer(seed=0) , dtype=tf.float32)\n",
    "b1 = tf.get_variable(\"b1\" , shape = [8] , initializer = tf.zeros_initializer)\n",
    "W2 = tf.get_variable(\"W2\" , shape = [5,5,8,16] , initializer =  tf.contrib.layers.xavier_initializer(seed=0) , dtype=tf.float32)\n",
    "b2 = tf.get_variable(\"b2\" , shape = [16] , initializer = tf.zeros_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = tf.nn.conv2d(X , W1 , [1,2,2,1] , 'VALID') + b1\n",
    "C1 = tf.contrib.layers.batch_norm(C1)\n",
    "Z1 = tf.nn.relu(C1)\n",
    "P1 = tf.nn.max_pool(Z1 , [1,2,2,1] , [1,2,2,1] , padding = 'VALID')\n",
    "\n",
    "C2 = tf.nn.conv2d(P1 , W2 , [1,2,2,1] , 'VALID') + b2\n",
    "C2 = tf.contrib.layers.batch_norm(C2)\n",
    "Z2 = tf.nn.relu(C2)\n",
    "P2 = tf.nn.max_pool(Z2 , [1,2,2,1] , [1,2,2,1] , padding = 'VALID')\n",
    "\n",
    "\n",
    "\n",
    "F = tf.contrib.layers.flatten(P2)\n",
    "FC1 = tf.contrib.layers.fully_connected(F , 24 , activation_fn = tf.nn.relu , biases_initializer = tf.zeros_initializer)\n",
    "FC2 = tf.contrib.layers.fully_connected(FC1 , 12 , activation_fn = None , biases_initializer = tf.zeros_initializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y , logits = FC2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_minibatch_valid(idx , size , xdata , ydata):\n",
    "    start = idx * size\n",
    "    directory = \"TrainR/\"\n",
    "    if(start + size >= len(xdata)):\n",
    "        end = len(xdata)\n",
    "    else:\n",
    "        end = start + size\n",
    "    x_batch = np.array(ImageOps.fit(Image.open(directory + xdata[start]) , (100,100) , Image.ANTIALIAS).convert('RGB')).reshape(1,100,100,3)\n",
    "    for ImgName in xdata[start+1:end]:\n",
    "        img = np.array(ImageOps.fit(Image.open(directory + ImgName) , (100,100) , Image.ANTIALIAS).convert('RGB')).reshape(1,100,100,3)\n",
    "        x_batch = np.concatenate((x_batch , img))\n",
    "    y_batch = ydata[start : end]\n",
    "        \n",
    "    return x_batch / 255.0 , y_batch*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "sess = tf.Session()\n",
    "output_softmax = tf.nn.softmax(FC2)\n",
    "corrects = tf.equal(tf.argmax(output_softmax, 1), tf.argmax(Y, 1))\n",
    "corrects_float = tf.reduce_sum(tf.cast(corrects , tf.float32))\n",
    "lr = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(60):\n",
    "    all_outputs = 0\n",
    "    for minibatch in range(len(Train_X) / 64):\n",
    "        Mini_X , Mini_Y = next_minibatch(minibatch , 64 , Train_X , Train_Y)\n",
    "        Mini_Y = Mini_Y.eval(session=sess)\n",
    "        _ , cost , outputs = sess.run([optimizer , cross_entropy , corrects_float] , feed_dict = {X : Mini_X , Y: Mini_Y})\n",
    "        all_outputs += outputs\n",
    "        if(minibatch == (len(Train_X) / 64) - 1):\n",
    "            valid_corrects = 0\n",
    "            for v in range(len(Valid_X)/64):\n",
    "                Mx , My = next_minibatch_valid(v , 64 , Valid_X , Valid_Y)\n",
    "                My = My.eval(session=sess)\n",
    "                valid_add = sess.run(corrects_float , feed_dict={X:Mx , Y:My})\n",
    "                valid_corrects += valid_add\n",
    "            Train_Accuracy = str(str((all_outputs / 3800.0) * 100))\n",
    "            Validation_Accuracy = str(str((valid_corrects / 950.0) * 100))\n",
    "            print \"Cost after epoch \" + str(epoch+1) + \" :  \" + str(cost) + \" , Train Accuracy : \" + Train_Accuracy + \"% , Validation Accuracy : \" + Validation_Accuracy + \"%\"\n",
    "    lr *= 0.999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
